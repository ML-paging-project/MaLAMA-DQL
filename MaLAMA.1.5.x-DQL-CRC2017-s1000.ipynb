{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21ddd393",
   "metadata": {},
   "source": [
    "# Machine Learning Augmented Green Paging with DQL and Compartmentalization on CRC2017 (s=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a76d5d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import networkx as nx\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac883b8",
   "metadata": {},
   "source": [
    "## LRU simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d73d1a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lru(cache_dict, cache_size, seq, start, box_width, hit_cost, miss_cost):\n",
    "    i = start\n",
    "    remain_width = box_width\n",
    "    while remain_width > 0:\n",
    "        if seq[i] in cache_dict.keys():\n",
    "            remain_width = remain_width - hit_cost\n",
    "            cache_dict.move_to_end(seq[i], last=False)\n",
    "        else:\n",
    "            remain_width = remain_width - miss_cost\n",
    "            cache_dict[seq[i]] = True\n",
    "            cache_dict.move_to_end(seq[i], last=False)\n",
    "            if len(cache_dict.keys()) > cache_size:\n",
    "                cache_dict.popitem(last=True)\n",
    "        i = i + 1\n",
    "        if i == len(seq):\n",
    "            break\n",
    "    return i  # Where the box ends/The position of the next request in the sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a647b6d3",
   "metadata": {},
   "source": [
    "## Build feature vector\n",
    "We use w (window_size) requests r[i-w], r[i-w+1], …, r[i-1] to build the feature vector of the i-th request r[i]. The length of the vector is w+4. \n",
    "When 1<=j<=w, the j-th variable of the vector is the frequency of the j-th most frequent page id among r[i-w], r[i-w+1], …, r[i-1]. \n",
    "We chop the w requests into four segments of length w/4. Count how many distinct ids in each segment, and put the countings in the last four variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65ceed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(seq, pointer_position, window_size):\n",
    "    vector = []\n",
    "    if pointer_position == 0:\n",
    "        for i in range(window_size + 4):\n",
    "            vector.append(0.00)\n",
    "    else:\n",
    "        if pointer_position < window_size:\n",
    "            window = seq[0:pointer_position]\n",
    "        else:\n",
    "            window = seq[pointer_position - window_size:pointer_position]\n",
    "        frequency = {}\n",
    "        distinct = {}\n",
    "        segs_distinct = [0.00, 0.00, 0.00, 0.00]  # chop the window into 4 segments\n",
    "        # count how many distinct page ids in each seg\n",
    "\n",
    "        i = 0\n",
    "        seg_id = 0\n",
    "        while i < len(window):\n",
    "            if window[i] in frequency.keys():\n",
    "                frequency[window[i]] = frequency[window[i]] + 1.00\n",
    "            else:\n",
    "                frequency[window[i]] = 1.00\n",
    "            distinct[window[i]] = True\n",
    "            if (i + 1) % (int(window_size / 4)) == 0:\n",
    "                segs_distinct[seg_id] = len(distinct.keys())\n",
    "                distinct = {}\n",
    "                seg_id = seg_id + 1\n",
    "            i = i + 1\n",
    "\n",
    "        if len(distinct.keys()) > 0:\n",
    "            segs_distinct[seg_id] = len(distinct.keys())\n",
    "\n",
    "        # 1<=j<=w, the j-th variable of the vector is the frequency of the j-th most frequent page id in\n",
    "        # the window\n",
    "        for v in frequency.values():\n",
    "            vector.append(v)\n",
    "        while len(vector) < window_size:\n",
    "            vector.append(0)\n",
    "        vector.sort(reverse=True)\n",
    "\n",
    "        # We chop the w requests into four segments of length w/4.\n",
    "        # Count how many distinct ids in each segment, and put the countings in the last four variables.\n",
    "        for v in segs_distinct:\n",
    "            vector.append(v)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7195ad",
   "metadata": {},
   "source": [
    "## Deep Q network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "245cb184",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, window_size, num_of_box_kind):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(window_size+4, 512, 1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv1d(256, 128, 1)\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        self.fc2 = nn.Linear(32, num_of_box_kind)\n",
    "        # print('.........'+str(num_of_box_kind))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(torch.transpose(x,1,2))\n",
    "        x = self.conv2(torch.transpose(x,1,2))\n",
    "        x = self.pool(torch.transpose(x,1,2))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773acda4",
   "metadata": {},
   "source": [
    "## Deep Q learning \n",
    "########## Cells below based on https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html# . \n",
    "I note my modification by comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "155514dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fad1488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maverick: I add a no_more_random. \n",
    "# In the final epochs of training, I can choose to ban the model from acting randomly.\n",
    "# Guarantee the model converge.\n",
    "def select_action(state, steps_done, EPS_START, EPS_END, \n",
    "                  EPS_DECAY, no_more_random, num_of_box_kinds, policy_net):\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold or no_more_random:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return torch.argmax(policy_net(state)), steps_done\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(num_of_box_kinds)]], device=device, dtype=torch.long), steps_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c7d557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(memory, BATCH_SIZE, ALPHA, GAMMA, policy_net, target_net, optimizer):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return policy_net, optimizer\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    #print(state_batch.shape)\n",
    "    state_action_values = policy_net(state_batch).gather(0, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    nsv=target_net(non_final_next_states).max(1)[0].detach()\n",
    "    lst=[]\n",
    "    for row in nsv:\n",
    "        lst.append(torch.argmax(row)*1.00)\n",
    "    next_state_values[non_final_mask] = torch.tensor(lst,device=device)\n",
    "    # Compute the expected Q values\n",
    "    \n",
    "    ##############################################\n",
    "    ## Maverick: I add learning rate ALPHA here ##\n",
    "    ##############################################\n",
    "    expected_state_action_values = ALPHA*((next_state_values * GAMMA) + reward_batch)\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    return policy_net, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76d12d3",
   "metadata": {},
   "source": [
    "########## Cells above based on https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html# . \n",
    "\n",
    "## Training epochs\n",
    "Train several models.\n",
    "<br>\n",
    "Pick the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "50735482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(sequence, seq_name):\n",
    "    features = []\n",
    "    for i in range(len(sequence)):\n",
    "        features.append([[get_vector(sequence, i, window_size)]])\n",
    "    features = torch.tensor(features, device=device)\n",
    "        \n",
    "    best_result = [math.inf for _ in range(num_episodes)]\n",
    "    best_hist = [0 for _ in range(number_of_box_kinds)]\n",
    "    print(len(best_result))\n",
    "    best_policy_net = DQN(window_size, number_of_box_kinds).to(device)\n",
    "    best_target_net = DQN(window_size, number_of_box_kinds).to(device)\n",
    "\n",
    "    for model in range(NUMBER_OF_MODELS):\n",
    "        steps_done = 0\n",
    "        policy_net = DQN(window_size, number_of_box_kinds).to(device)\n",
    "        target_net = DQN(window_size, number_of_box_kinds).to(device)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        target_net.eval()\n",
    "\n",
    "        optimizer = optim.RMSprop(policy_net.parameters())\n",
    "        memory = ReplayMemory(10000)\n",
    "        result = []\n",
    "        hist = [0 for _ in range(number_of_box_kinds)]\n",
    "\n",
    "        for i_episode in range(num_episodes):\n",
    "            global_lru = collections.OrderedDict()\n",
    "            for xx in range(number_of_box_kinds):\n",
    "                hist[xx] = 0\n",
    "            pointer = 0\n",
    "            impact = 0\n",
    "            while pointer < len(sequence):\n",
    "                startpointer = pointer\n",
    "                state = features[pointer]\n",
    "                # state = torch.tensor([[state]], device=device)\n",
    "                state = torch.transpose(state,1,2)\n",
    "                action, steps_done = select_action(state, steps_done, EPS_START, EPS_END, EPS_DECAY,\n",
    "                                                   (i_episode + 1) / num_episodes > 0.9, number_of_box_kinds,\n",
    "                                                   policy_net)\n",
    "                box_id = action\n",
    "                # print(box_id)\n",
    "                hist[box_id] = hist[box_id] + 1\n",
    "                cache_size = 2 ** box_id\n",
    "                box_width = miss_cost * cache_size\n",
    "\n",
    "                # Compartmentalization\n",
    "                # Load top pages from LRU stack.\n",
    "                mycache = collections.OrderedDict()\n",
    "                for pid in global_lru.keys():\n",
    "                    mycache[pid] = True\n",
    "                    mycache.move_to_end(pid, last=True)  ###########\n",
    "                    if len(mycache) == cache_size:\n",
    "                        break\n",
    "\n",
    "                action = torch.tensor([[[action]]], device=device)  # make it a tensor\n",
    "                pointer = run_lru(mycache, cache_size, sequence, pointer, box_width, 1, miss_cost)\n",
    "                endpointer = pointer\n",
    "\n",
    "                # Update global stack\n",
    "                for x in range(startpointer, endpointer):\n",
    "                    if sequence[x] in global_lru.keys():\n",
    "                        global_lru.move_to_end(sequence[x], last=False)\n",
    "                    else:\n",
    "                        global_lru[sequence[x]] = True\n",
    "                        global_lru.move_to_end(sequence[x], last=False)\n",
    "\n",
    "                # print(mi)\n",
    "                area = 3 * miss_cost * cache_size * cache_size\n",
    "                impact = impact + area\n",
    "                reward = torch.tensor([[[-area]]], device=device)\n",
    "\n",
    "                if pointer < len(sequence):\n",
    "                    next_state = features[pointer]\n",
    "                    next_state = torch.transpose(next_state, 1, 2)\n",
    "                else:\n",
    "                    next_state = None\n",
    "\n",
    "                # Store the transition in memory\n",
    "                memory.push(state, action, next_state, reward)\n",
    "\n",
    "                # Move to the next state\n",
    "                state = next_state\n",
    "\n",
    "                # Perform one step of the optimization (on the policy network)\n",
    "                policy_net, optimizer = optimize_model(memory, BATCH_SIZE, ALPHA, GAMMA, policy_net, \n",
    "                                                       target_net, optimizer)\n",
    "\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            clear_output()\n",
    "            print(seq_name)\n",
    "            print(best_result[-1])\n",
    "            print('MODEL-' + str(model))\n",
    "            print('epoch=' + str(i_episode) + '..........impact=' + str(impact))\n",
    "            result.append(impact.item())\n",
    "\n",
    "            if i_episode % TARGET_UPDATE == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        print('Complete')\n",
    "        if result[-1] < best_result[-1]:\n",
    "            for idx in range(len(result)):\n",
    "                best_result[idx] = result[idx]\n",
    "            for idx in range(len(hist)):\n",
    "                best_hist[idx] = hist[idx]\n",
    "\n",
    "            # store the best for further use\n",
    "            best_policy_net.load_state_dict(policy_net.state_dict())\n",
    "            best_target_net.load_state_dict(target_net.state_dict())\n",
    "\n",
    "    print(best_hist)\n",
    "    return best_policy_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3966b9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(sequence, policy_net):\n",
    "    features = []\n",
    "    for i in range(len(sequence)):\n",
    "        features.append([[get_vector(sequence, i, window_size)]])\n",
    "    features = torch.tensor(features, device=device)\n",
    "    \n",
    "    global_lru = collections.OrderedDict()\n",
    "    pointer = 0\n",
    "    impact = 0\n",
    "    while pointer < len(sequence):\n",
    "        startpointer = pointer\n",
    "        state = features[pointer]\n",
    "        # state = torch.tensor([[state]], device=device)\n",
    "        state = torch.transpose(state, 1, 2)\n",
    "        steps_done=0\n",
    "        action, steps_done = select_action(state, steps_done, EPS_START, EPS_END, EPS_DECAY,\n",
    "                                           True, number_of_box_kinds, policy_net)\n",
    "        box_id = action\n",
    "        # print(box_id)\n",
    "        # hist[box_id] = hist[box_id] + 1\n",
    "        cache_size = 2 ** box_id\n",
    "        box_width = miss_cost * cache_size\n",
    "\n",
    "        # Compartmentalization\n",
    "        # Load top pages from LRU stack.\n",
    "        mycache = collections.OrderedDict()\n",
    "        for pid in global_lru.keys():\n",
    "            mycache[pid] = True\n",
    "            mycache.move_to_end(pid, last=True)  ###########\n",
    "            if len(mycache) == cache_size:\n",
    "                break\n",
    "\n",
    "        action = torch.tensor([[[action]]], device=device)  # make it a tensor\n",
    "        pointer = run_lru(mycache, cache_size, sequence, pointer, box_width, 1, miss_cost)\n",
    "        endpointer = pointer\n",
    "\n",
    "        # Update global stack\n",
    "        for x in range(startpointer, endpointer):\n",
    "            if sequence[x] in global_lru.keys():\n",
    "                global_lru.move_to_end(sequence[x], last=False)\n",
    "            else:\n",
    "                global_lru[sequence[x]] = True\n",
    "                global_lru.move_to_end(sequence[x], last=False)\n",
    "\n",
    "        # print(mi)\n",
    "        area = 3 * miss_cost * cache_size * cache_size\n",
    "        impact = impact + area\n",
    "    return impact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e182da",
   "metadata": {},
   "source": [
    "## Random box picking\n",
    "Randomly and uniformly pick one box when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ff89b282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_pick(sequence):\n",
    "    total_impact = 0\n",
    "    rounds = 1000\n",
    "    for _ in range(rounds):\n",
    "        pointer = 0\n",
    "        global_lru = collections.OrderedDict()\n",
    "        while pointer < len(sequence):\n",
    "            startpointer = pointer\n",
    "            box = random.randint(0, number_of_box_kinds - 1)\n",
    "            cache_size = 2 ** box\n",
    "            box_width = miss_cost * cache_size\n",
    "            # Compartmentalization\n",
    "            # Load top pages from LRU stack.\n",
    "            mycache = collections.OrderedDict()\n",
    "            for pid in global_lru.keys():\n",
    "                mycache[pid] = True\n",
    "                mycache.move_to_end(pid, last=True)  ###########\n",
    "                if len(mycache) == cache_size:\n",
    "                    break\n",
    "\n",
    "            pointer = run_lru(mycache, cache_size, sequence, pointer, box_width, 1, miss_cost)\n",
    "            endpointer = pointer\n",
    "\n",
    "            # Update global stack\n",
    "            for x in range(startpointer, endpointer):\n",
    "                if sequence[x] in global_lru.keys():\n",
    "                    global_lru.move_to_end(sequence[x], last=False)\n",
    "                else:\n",
    "                    global_lru[sequence[x]] = True\n",
    "                    global_lru.move_to_end(sequence[x], last=False)\n",
    "\n",
    "            mi = 3 * miss_cost * cache_size * cache_size\n",
    "            total_impact = total_impact + mi\n",
    "    print(total_impact / rounds)\n",
    "    return total_impact / rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df44f9fc",
   "metadata": {},
   "source": [
    "## Michael Bender's algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "befce684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def michael(sequence):\n",
    "    # Michael's algorithm \n",
    "    MAXBOX = number_of_box_kinds - 1\n",
    "    countings = [0 for i in range(number_of_box_kinds)]\n",
    "    countings[0] = 1\n",
    "    currentbox = 0\n",
    "    pointer = 0\n",
    "    total_impact = 0\n",
    "    global_lru = collections.OrderedDict()\n",
    "    while pointer < len(sequence):\n",
    "        startpointer = pointer\n",
    "        cache_size = 2 ** currentbox\n",
    "        box_width = miss_cost * cache_size\n",
    "        # Compartmentalization\n",
    "        # Load top pages from LRU stack.\n",
    "        mycache = collections.OrderedDict()\n",
    "        for pid in global_lru.keys():\n",
    "            mycache[pid] = True\n",
    "            mycache.move_to_end(pid, last=True)  ###########\n",
    "            if len(mycache) == cache_size:\n",
    "                break\n",
    "\n",
    "        pointer = run_lru(mycache, cache_size, sequence, pointer, box_width, 1, miss_cost)\n",
    "        endpointer = pointer\n",
    "\n",
    "        # Update global stack\n",
    "        for x in range(startpointer, endpointer):\n",
    "            if sequence[x] in global_lru.keys():\n",
    "                global_lru.move_to_end(sequence[x], last=False)\n",
    "            else:\n",
    "                global_lru[sequence[x]] = True\n",
    "                global_lru.move_to_end(sequence[x], last=False)\n",
    "\n",
    "        mi = 3 * miss_cost * cache_size * cache_size\n",
    "        total_impact = total_impact + mi\n",
    "        if currentbox == MAXBOX:\n",
    "            currentbox = 0\n",
    "        elif countings[currentbox] % 4 == 0:\n",
    "            currentbox = currentbox + 1\n",
    "        else:\n",
    "            currentbox = 0\n",
    "        countings[currentbox] = countings[currentbox] + 1\n",
    "\n",
    "    michael = [total_impact for hhh in range(num_episodes)]\n",
    "    print(total_impact)\n",
    "    return total_impact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feca2f07",
   "metadata": {},
   "source": [
    "## Offline opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9aa4dade",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# run lru, mark down hits and faults\n",
    "def LRU(sequence, size):\n",
    "    stack = collections.OrderedDict()\n",
    "    marks = []\n",
    "    for r in sequence:\n",
    "        if r in stack.keys():\n",
    "            marks.append(True)  # a hit\n",
    "        else:\n",
    "            if len(stack.keys()) == size:  # memory is full\n",
    "                stack.popitem(last=True)\n",
    "            marks.append(False)  # a fault\n",
    "            stack[r] = True\n",
    "        stack.move_to_end(r, last=False)\n",
    "    return marks\n",
    "\n",
    "def opt(sequence):\n",
    "    ############################\n",
    "    # get the request_sequence\n",
    "    request_sequence = sequence\n",
    "    \n",
    "    # parameters\n",
    "    n = len(request_sequence)\n",
    "    s = miss_cost  # time to handle fault\n",
    "    k = 2**(number_of_box_kinds-1)  # largest memory size\n",
    "    \n",
    "    ############################\n",
    "    print('building dag...')\n",
    "    # find end indexes\n",
    "    # initialize table\n",
    "    end_index = [[0 for i in range(n)] for j in range(number_of_box_kinds)]\n",
    "    # find end indexes\n",
    "    for i in range(number_of_box_kinds):\n",
    "        memory_size = math.floor(k / (2 ** i))\n",
    "        # run lru on the whole sequence\n",
    "        # with memory_size = k / (2 ** i)\n",
    "        is_a_hit = LRU(request_sequence, memory_size)\n",
    "        # find the end index for node[i][0]\n",
    "        box_width = s * memory_size\n",
    "        request_position = 0\n",
    "        actual_running_time = 0\n",
    "        while actual_running_time < box_width and request_position < n:\n",
    "            if is_a_hit[request_position]:\n",
    "                actual_running_time += 1\n",
    "            else:\n",
    "                actual_running_time += s\n",
    "            request_position += 1\n",
    "        end_index[i][0] = request_position - 1\n",
    "        # find end indexes for other nodes in row i\n",
    "        for j in range(1, n):\n",
    "            if is_a_hit[j - 1]:\n",
    "                actual_running_time = actual_running_time - 1\n",
    "                if actual_running_time < box_width:\n",
    "                    end_index[i][j] = min(end_index[i][j - 1] + 1, n - 1)\n",
    "                else:\n",
    "                    end_index[i][j] = end_index[i][j - 1]\n",
    "            else:\n",
    "                actual_running_time = actual_running_time - s\n",
    "                spared_time = box_width - actual_running_time\n",
    "                request_position = end_index[i][j - 1] + 1\n",
    "                while request_position < n and spared_time > 0:\n",
    "                    if is_a_hit[request_position]:\n",
    "                        spared_time -= 1\n",
    "                    else:\n",
    "                        spared_time -= s\n",
    "                    request_position += 1\n",
    "                end_index[i][j] = request_position - 1\n",
    "            if end_index[i][j] > end_index[i][j - 1]:\n",
    "                for ii in range(1 + end_index[i][j - 1], 1 + end_index[i][j]):\n",
    "                    if is_a_hit[ii]:\n",
    "                        actual_running_time += 1\n",
    "                    else:\n",
    "                        actual_running_time += s\n",
    "    \n",
    "    #########################################\n",
    "    # turn the table to dag\n",
    "    dag = nx.Graph()\n",
    "    for i in range(number_of_box_kinds):\n",
    "        dag.add_edge('start', str(i) + '-0', weight=0)\n",
    "    for i in range(number_of_box_kinds):\n",
    "        for j in range(n):\n",
    "            if end_index[i][j] < (n - 1):\n",
    "                for ii in range(number_of_box_kinds):\n",
    "                    dag.add_edge(str(i) + '-' + str(j),\n",
    "                                 str(ii) + '-' + str(end_index[i][j] + 1),\n",
    "                                 weight=3 * s * (k / (2 ** i)) ** 2)\n",
    "            else:\n",
    "                dag.add_edge(str(i) + '-' + str(j), 'end',\n",
    "                             weight=3 * s * (k / (2 ** i)) ** 2)\n",
    "    print('searching the shortest path...')\n",
    "    p = nx.dijkstra_path(dag, source='start', target='end')\n",
    "    d = nx.dijkstra_path_length(dag, source='start', target='end')\n",
    "    print(p)\n",
    "    print(d)\n",
    "    opt=[d for _ in range(num_episodes)]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "60f0f74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_seq(file):\n",
    "    sequence = []\n",
    "    with open(file, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip('\\n')\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            data=line.split(',')\n",
    "            sequence.append(data[0])\n",
    "    f.close()\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "39b99c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0x41aa80', '0x41aafd', '0x41aa6e', '0x41b785', '0x418c7f']\n"
     ]
    }
   ],
   "source": [
    "seq = read_seq('crc2017/cactusadm_train.csv')\n",
    "print(seq[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2948220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "GAMMA = 0.3\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.00001\n",
    "EPS_DECAY = 500\n",
    "TARGET_UPDATE = 5\n",
    "window_size=256\n",
    "miss_cost = 1000 ##################\n",
    "# number_of_box_kinds = min(8,math.ceil(math.log2(miss_cost))) #############\n",
    "number_of_box_kinds=8\n",
    "NUMBER_OF_MODELS = 1 # Train several models, choose the best\n",
    "num_episodes = 10\n",
    "ALPHA = 0.8 # Learning rate #########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "afc015d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cactusadm\n",
      "inf\n",
      "MODEL-0\n",
      "epoch=9..........impact=tensor(3929088000, device='cuda:0')\n",
      "Complete\n",
      "[0, 0, 0, 0, 0, 1279, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "best_net=train_models(seq, 'cactusadm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "40ac954c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4231872000, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(test_model(seq, best_net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8148ff02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building dag...\n",
      "searching the shortest path...\n"
     ]
    }
   ],
   "source": [
    "print(opt(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440c6552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a988df38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97da822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea0d689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7943e1f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
